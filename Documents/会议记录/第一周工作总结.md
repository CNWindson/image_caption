

# 第一周工作总结

## 成员信息统计

组长：樊迪超

组员：刘茜茜，余蓉，柳琦晗

## 1.选题背景与项目目标

### 1.1项目目标

**基本要求：** 看图说话（Image Caption)任务是结合CV和NLP两个领域的一种比较综合的任务，Image Caption模型的输入是一幅图像，输出是对该幅图像进行描述的一段文字。这项任务要求模型可以识别图片中的物体、理解物体间的关系，并用一句自然语言表达出来。

**进阶想法：**加强图片与文字描述的联系，是否可以实现互相转化？是否可以实现按需描述？

### 1.2选题背景

image caption在应用方面有很多的场景：

1.为照片匹配合适的文字，方便检索或省去用户手动配字。

2.帮助视觉障碍者去理解图像内容。

3.在艺术创作和罪犯画像等领域也有应用。



## 2.资料查阅

### 2.1 基础论文

1.Show and Tell A Neural Image Caption Generator1411.4555
2.Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation1406.1078

### 2.2 进阶方法论文

1.Recurrent neural network regularization
2.What Value Do Explicit High Level Concepts Have in Vision to Language Problems1506.01144
3.Show, Attend and Tell Neural Image Caption Generation with Visual Attention1502.030442017 
4.Attention is all you need
5.Top down and bottom up attention for image captioning
6.Natural baby talk

> 主要复现show and tell一文中的cnn+rnn模型，进阶方法考虑正则化&attention&babytalk&替换目标函数等。



## 3.Show and Tell 模型理解

### 3.1基本模型构架

NIC模型的结构非常“清晰”：

利用encoder-decoder框架

1.首先利用CNN（这里是inceptionv3）作为encoder，生成feature map

2.feature map作一次embeding全连接，生成长度为512的一维向量。

3.再使用LSTM作为decoder，将向量输入decoder。

4.模型训练就是使用最大化对数似然来训练，然后在测试阶段采用beam search来减小搜索空间。

> 与传统的lstm网络的区别在于，图像特征不是每一时刻都有输入，而只在第一时刻给入，作者给出的理由是，如果在每个时刻都输入图像特征，那么模型会把图像的噪声放大，并且容易过拟合。

![](/home/liuqihan/文档/看图说话机器人项目检测/404 Not Found/第一周演示ppt/encoder-decoder.png)



### 3.2模型目标函数

Image Caption任务的训练过程可以描述为这个形式：

对于训练集的一张图片 I,其对应的描述为序列 S={S1,S2,...}（其中 Si代表句子中的词）。对于模型 θ ，给定输入图片I，模型生成序列 S的概率为连乘形式的似然函数。

将似然函数取对数，得到对数似然函数：

$logP(S|I;θ)=∑_{t=0}^NlogP(St|S0,S1,...,St−1,I;θ)$

模型的训练目标就是最大化全部训练样本的对数似然之和：

$θ∗=argmaxθ∑(I,S)logP(S|I;θ)$

> 一些思考：虽然训练是用的最大化后验概率，但是在评估时使用的测度则为BLEU，METEOR，ROUGE，CIDER等。
>
> 这里有训练loss和评估方法不统一的问题。而且log似然可以认为对每个单词都给予一样的权重，然而实际上有些单词可能更重要一些（比如说一些表示内容的名词，动词，形容词）。



### 3.3 BeamSearch搜索算法

模型在最后的inference中使用了beam search。

传统的搜索算法中，随着词序列的增长，每个位置都有一个词表级别的序列要遍历，找到所有序列再挑出最优解的计算量就太大了，beam search（集束搜索）则选择了概率最大的前k个。这个k值也叫做集束宽度（Beam Width）。也是这个搜索算法的唯一参数。如果一个序列深度有10000，我们取k=3 可以得到30000句话，选择前三个最优解。

> 一点思考：关于自然语言，如果用贪婪算法找到评分最高的解，不一定是最优解，要考虑到语言的局部相关性更大，而不是全局最优，这点上，beam search 虽然可能在剪枝的过程中丢弃了评分最高的解，但更大概率找到最合适的解。

![](/home/liuqihan/文档/看图说话机器人项目检测/404 Not Found/第一周演示ppt/beam search.png)



### 3.4评价指标

**BLEU** **（BiLingual Evaluation Understudy）**

3.4.1为了解决**“百搭”**词的问题，需要修正 precision 的计算方式。

考虑模型生成的句子 c的全部 n-gram ，考察其中的任一 n-gram ：首先计算其在 c,中出现的次数 Count(n-gram),然后统计其在各参考句子中分别出现的次数的最大值，将该值与 Count(n-gram)的较小者记作该 n-gram 的匹配次数 Countclip(n-gram) 。之后，再把每个 n-gram 的计算结果累加起来，得到句子的结果。所以precision可以用如下方式计算：

![](/home/liuqihan/文档/看图说话机器人项目检测/404 Not Found/第一周演示ppt/百搭.png)

3.4.2解决输出短句倾向

对短句子进行惩罚，加入惩罚因子。

![](/home/liuqihan/文档/看图说话机器人项目检测/404 Not Found/第一周演示ppt/2018-11-10 16-10-58屏幕截图.png)

最后得出表达式：

![](/home/liuqihan/文档/看图说话机器人项目检测/404 Not Found/第一周演示ppt/bleu.png)

## 4.数据准备

由于网络上多数数据集均为英文描述，因此我们也使用英文描述的数据集Flickr和MSCOCO。

![1541998957369](/home/liuqihan/.config/Typora/typora-user-images/1541998957369.png)

## 5.项目计划

### 5.1 Show And Tell 模型复现

#### 5.1.1 制作tfrecord

使用im2txt模型中的build coco模型生成数据集的tfrecord，其中metadata包含图片id，图片名称，以及图片的五句话描述。同事生成词频表，舍弃频率小于4的单词，小于4的词频赋予一个新的类——unk。

#### 5.1.2 模型训练

使用tran.py对数据进行训练，对比结果如下：

Flickr8k：

![1541999290443](/home/liuqihan/.config/Typora/typora-user-images/1541999290443.png)

step = 30000
Perplexity = 15.781156



MSCOCO：

![1541999428137](/home/liuqihan/.config/Typora/typora-user-images/1541999428137.png)

step = 200000
Perplexity = 9.191941 

两个数据集loss对比如下：

![1541999496106](/home/liuqihan/.config/Typora/typora-user-images/1541999496106.png)

其中上图为mscoco，下图为Flickr8k，可以看出二者loss相差不多，甚至下图loss要更小，但实际结果来看，mscoco数据集的结果要更好，分析原因可能由于mscoco数据集数量更大，分类更详细，在同样的loss下，模型却学到了更多的东西。

#### 5.1.3遇到的问题

遇到的问题
##### tfrecord 图片文件读取方式不对
这里读取的 r要改为rb

```python
with tf.gfile.FastGFile(image.filename, "rb") as f:
encoded_image = f.read()
```

否则会出现如下报错

```python
UnicodeDecodeError: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte
```

##### tfrecord 制作时的 windows 下编码格式问题
linux 下面不会出现 windows 下面会报错

```python
tensorflow.python.framework.errors_impl.NotFoundError: NewRandomAccessFile failed to Create/Open:
E:\flick\data\Flickr_8k.trainImages.txt :
εͳ\udcd5Ҳ\udcbb\udcb5\udcbdָ \udcb6\udca8\udcb5\udcc4·\udcbe\udcb6\udca1\udca3 ; No such process
```

这段代码:

```python
def _bytes_feature(value):
return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))
```

需要对 value 进行编码 而不是直接 str 应该改成:

```python
def _bytes_feature(value):
return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.encode()if
type(value)==str else value]))
```

这里要根据值得类型进行判断 如果不是 str的类型要进行encode 编码

##### 梯度爆炸问题
化器选择问题出错导致 loss直线上升，在更换为sgd优化器后loss表现正常。

##### 制作tf record 速度问题

在制作 tfrecord 的 namedTuple 时,读取 caption_file 中 会使用一句话

```python
with tf.gfile.FastGFile(captions_file, "r") as fc:
lines = fc.readlines()
id_to_captions = {}
for line in lines:
ll = line.split("\t")
filename = ll[0].split("#")[0]
caption = ll[1]
if filename in filenames:
id_to_captions.setdefault(filename, [])
id_to_captions[filename].append(caption)
```

用来把 每个图片对应的语句给放到 id_to_captions 的字典中
filenames 是一个列表
if filename in filenames 速度非常慢,而且外循环 for line in lines 造成速度慢的原因。
把filenames 这个列表 转化为 filenames_str 的 字符串 使用逗号隔开文件名, 然后使用if filenames.find(filename) != -1 去 替换掉 if filename in filenames 速度将会大大加快,这在制作flickr30k 的时候尝试过。

### 5.2后续计划

典型的image caption模型采样LSTM，问题是过分依赖language model, 使得caption经常与图像内容关联不够；（而在深度学习之前，典型做法是更依赖图像内容，而对language model关注不够，例如采用一系列视觉检测器检测图像内容，然后基于模板或者其他方式生成caption。）

所以我们考虑是否可以尝试减少对语言模型的依赖，更多地结合图像内容。

一是cnn网络学习过程中，能否尽量保留图像信息，对学习到的分类进行保留，提高特征选择部分和语言模型部分的关联性。

二是调整模型参数，是否可以改变目标函数等。

针对这些想法提出以下改进方案：

#### 5.2.1替换cnn模型

模型中使用考虑将此模型中的CNN换成InceptionV4、Densenet、Resnet，以提高CNN对图片特征的学习能力。

#### 5.2.2 加入高层语义模型

CNN最后的分类层可以学习得到大量诸如"图中有没有人"、"图中有没有猫"类似的高层信息，这种高层于一与最终生成的语句非常相关。

简单来说，假设要从图中找到$c$类物体，那么就加入使用$c$个Softmax层进行分类。

在训练时，从所有描述中提取出现最频繁的$c$个单词作为CNN的总标签，每个图片的训练标签直接从其描述语句中获取，即描述语句中出现了哪些标签；训练完成后，可以针对每张图片提取高层的语义表达向量$V_{att}(I)$，代替$CNN(I)$作为RNN的输入。

#### 5.2.3 加入attention机制

在提取图像特征后，将图像特征和之前的预测出的单词信息共同输入RNN中计算隐层输出，

根据之前预测出的单词信息来提示应该关注图像的哪个部分，而不是漫无目的关注整张图像。

（比如我们之前时刻预测出了“ride”这个单词，那么接下来应该预测的可能是horse，bike这种单词，而不是盲目的关注一些毫不相关的图像区域）。

> CNN接入RNN的方式可以选择只在第一个时刻输入图像特征或在每个时刻都输入图像特征,这里要使用attention就必须使用第二种方式来链接cnn与rnn模型。另外考虑到过多图像数据的干预，可能导致模型过拟合，要考虑加入正则化或者其他惩罚机制。

#### 5.2.4使用baby talk模型

采用物体检测器检测图像中的物体(visual words)，然后在每个word的生成时刻，自主决定选取text word（数据集中的词汇） 还是 visual word（检测到的词汇）。

（对于一张输入的图片，将物体检测的结果和CNN中间层的features同时输入带有Attention的RNN里）

#### 5.2.5替换目标函数

大体的想法就是用REINFORCE with  baseline来希望直接优化BLEU4分数。具体训练的时候，先用最大似然方法做预训练，然后用REINFORCE  finetune。

在REINFORCE阶段，生成器不再使用任何ground  truth信息，而是直接从RNN模型随机采样，最后获得采样的序列的BLEU4的分数r作为reward来更新整个序列生成器。

> baby talk一文中使用的baseline在每个时刻是不同的；是每个RNN隐变量的一个线性函数。这个线性函数也会在训练中更新。文中的系统最后能比一般的的cross extropy loss，和scheduled sampling等方法获得更好的结果